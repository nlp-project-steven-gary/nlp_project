{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    NLP group project - garys code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the urls from the GitHub trending sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GitHub_trend(url):\n",
    "    url = 'https://github.com/trending'\n",
    "    headers = {'User-Agent': 'Codeup Ada Data Science'} # codeup.com doesn't like our default user-agent\n",
    "    response = get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.title\n",
    "    #  pull the GitHub Trending repositories\n",
    "    trending = soup.find_all('div', class_= 'd-inline-block')\n",
    "    #  find all the \"hrefs\", which are the trending titles\n",
    "    re.findall(r'href=\"(.*?)\"', str(trending))\n",
    "    return trending\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . turn this off for now, until access limit is removed\n",
    "#  trending = get_GitHub_trend('https://github.com/trending')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scrape the urls from the GitHub sites with the most stars (decending order of stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape GitHub repositories with the most stars\n",
    "def get_GitHub_most_stars(url = 'https://github.com/search?o=desc&q=stars%3A%3E0&s=stars&type=Repositories'):\n",
    "    headers = {'User-Agent': 'Codeup Ada Data Science'} \n",
    "    response = get(url, headers=headers)\n",
    "    contents = response.content\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the html file from a file in the curent working directory ('GitHubTopStars_repos.html')\n",
    "def read_GitHub_most_stars(filename):\n",
    "    with open(filename) as f:\n",
    "        contents = f.read()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_urls(contents):\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    title = soup.title\n",
    "    orig_text = soup.text\n",
    "    stars = soup.find_all('a', class_= 'v-align-middle')\n",
    "    #  find all the \"hrefs\", which are the trending titles\n",
    "    return(re.findall(r'href=\"(.*?)\"', str(stars)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  turn off the web access for now and just read the cached files from the working directory\n",
    "#  contents = get_GitHub_most_stars()\n",
    "list_of_html_files = ['GitHubTopStars_repos-page1.html','GitHubTopStars_repos-page2.html',\\\n",
    "                      'GitHubTopStars_repos-page3.html','GitHubTopStars_repos-page4.html',\\\n",
    "                      'GitHubTopStars_repos-page5.html','GitHubTopStars_repos-page6.html',\\\n",
    "                      'GitHubTopStars_repos-page7.html','GitHubTopStars_repos-page8.html',\\\n",
    "                      'GitHubTopStars_repos-page9.html','GitHubTopStars_repos-page10.html',\\\n",
    "                      'GitHubTopStars_repos-page11.html','GitHubTopStars_repos-page12.html',\\\n",
    "                      'GitHubTopStars_repos-page13.html']\n",
    "\n",
    "star_list = []\n",
    "for html_file in list_of_html_files:\n",
    "    contents = read_GitHub_most_stars(html_file)\n",
    "    next_list = parse_urls(contents)\n",
    "    star_list = star_list + next_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_base_url = 'https://github.com'\n",
    "\n",
    "git_repo_url = []\n",
    "\n",
    "for repo in star_list:\n",
    "    git_repo_url.append(github_base_url + repo)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk = get_git_info(star_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk = drop_empty_readmes(bulk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
